import logging
import re
import os

import requests
import pandas as pd
from tqdm import tqdm

from config.constants import TOPICS_URLS_JSON, TOPICS_ORES_SCORES_JSON
from src.utils import load_json, dump_json

import argparse

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(levelname)-8s : %(message)s")


def print_stats(df):
    num_high_quality = df["predicted_class"].isin(["B", "GA", "FA"]).sum()
    percentage_high_quality = (num_high_quality / len(df)) * 100

    print(
        f"Finish successfully!\n"
        f"- File saved to: {TOPICS_ORES_SCORES_JSON}\n"
        f"- {num_high_quality} out of {len(df)} Topics "
        f"({percentage_high_quality:.2f}%) are of high-quality class B, GA, or FA."
    )


def print_warning():

    raise FileNotFoundError(
        f"""
        Error: The required file '{TOPICS_URLS_JSON}' does not exist.

        Please ensure that the file is generated by running:
        get_wikipedia_urls.py

        Once the file is created, re-run this script.
        """
    )


def get_most_edited_wikipedia_titles(year: str, month: str, day: str = "all-days"):
    a = requests.get(
        f"https://wikimedia.org/api/rest_v1/metrics/edited-pages/top-by-edits/en.wikipedia/all-editor-types/content/{year}/{month}/{day}"
    )
    results = a.json()["items"][0]["results"][0]["top"]
    titles = [result["page_title"].replace("_", " ") for result in results]
    return titles


def get_html_content(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"Error fetching the URL: {e}")
        return None


def call_ORES_api(revids, verbose=False):
    """
    Example: https://en.wikipedia.org/w/index.php?title=Zagreb_Film&oldid=1182715485
    Put that into ORES > https://ores.wikimedia.org/v3/scores/enwiki?models=articlequality&revids=1182715485
    Return format:
    {"enwiki": {"models": {"articlequality": {"version": "0.9.2"}},
    "scores": {
      "1182715485": {
        "articlequality": {
          "score": {
            "prediction": "C",
            "probability": {"B": 0.13445393715303733, "C": 0.4728322988805659, "FA": 0.004610104723503904,
            "GA": 0.048191603091353855, "Start": 0.3326359821017828, "Stub": 0.007276074049756365}
          }
        }
      }}}}

    Note: Predicted scores for ORES API are as follows:
          Stub, Start, C, B, GA, FA
    For quality, only B-class or higher are considered.
    """
    base_url = "https://ores.wikimedia.org/v3/scores/enwiki"
    params = {"models": "articlequality", "revids": revids}

    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()  # Raise an exception for HTTP errors
        json_response = response.json()
        if verbose:
            print(f"Response for revids {revids}: {json_response}")
        return json_response["enwiki"]["scores"][f"{revids}"]["articlequality"]["score"]
    except requests.RequestException as e:
        print(f"API request error: {e}")
    except KeyError as ke:
        print(f"KeyError for revids {revids}: {ke}")
        print(f"Full response: {json_response}")
    return None


def get_predicted_quality(title, verbose=False):
    url = f"https://en.wikipedia.org/w/index.php?title={title}&action=history"
    html_content = get_html_content(url)
    if html_content is None:
        logger.error(f"Cannot get the content of {url}")
        return None
    match = re.search(r'"wgCurRevisionId":(\d+)', html_content)
    if match:
        revids = match.group(1)
    else:
        logger.error(f"Cannot find revids for {title}.")
        return None

    predicted_quality = call_ORES_api(revids, verbose=verbose)
    return predicted_quality


def main(args):

    if not os.path.exists(TOPICS_URLS_JSON):
        print_warning()
        return

    data = {"topic": [], "url": [], "predicted_class": [], "predicted_scores": []}

    topics_with_urls_dict = load_json(TOPICS_URLS_JSON)
    for topic, url in tqdm(topics_with_urls_dict.items(), desc="Processing topics"):
        if url:
            url_topic = url.split("/wiki/")[-1]
            predicted_quality = get_predicted_quality(url_topic, verbose=False)
            if predicted_quality is None:
                logger.error(f'Fail to include "{topic}"')
                continue
            data["topic"].append(topic)
            data["url"].append(url)
            data["predicted_class"].append(predicted_quality["prediction"])
            data["predicted_scores"].append(predicted_quality["probability"])

    df = pd.DataFrame(data)
    print_stats(df)
    if args.extract_HQ_articles:
        high_quality_classes = {"B", "GA", "FA"}
        df = df[df["predicted_class"].isin(high_quality_classes)]

    df.to_csv('topics_ores_scores.csv', index=False)

    records = df.to_dict(orient="records")
    dump_json(records, TOPICS_ORES_SCORES_JSON, ensure_ascii=False)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Get ORES scores for Wikipedia articles."
    )
    parser.add_argument(
        "--extract-HQ-articles",
        action="store_true",
        default=True,
        help="Extract high-quality Wikipedia articles based on ORES scores.",
    )
    main(parser.parse_args())
