import logging
import re
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
import requests
import pandas as pd
from tqdm import tqdm
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

from config.constants import TOPICS_URLS_JSON, DATA_DIR
from src.utils import load_json, dump_json, format_args
import argparse

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(levelname)-8s : %(message)s")


def create_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=5,  # Increased retries
        backoff_factor=2,  # Increased backoff
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(
        max_retries=retry_strategy,
        pool_connections=25,  # Reduced connections
        pool_maxsize=25,
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session


def print_stats(df):
    num_high_quality = df["predicted_class"].isin(["B", "GA", "FA"]).sum()
    percentage_high_quality = (num_high_quality / len(df)) * 100

    print(
        f"Finish successfully!\n"
        f"- {num_high_quality} out of {len(df)} Topics "
        f"({percentage_high_quality:.2f}%) are of high-quality class B, GA, or FA."
    )


def print_warning(topics_urls_json):
    if not os.path.exists(topics_urls_json):
        raise FileNotFoundError(
            f"""
            Error: The required file '{topics_urls_json}' does not exist.
            Please ensure that the file is generated by running:
            get_wikipedia_urls.py
            Once the file is created, re-run this script.
            """
        )


def get_html_content(session, url):
    try:
        time.sleep(0.5)  # Add delay between requests
        response = session.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logger.error(f"Error fetching the URL {url}: {e}")
        return None


def call_ORES_api(session, revids, verbose=False):
    base_url = "https://ores.wikimedia.org/v3/scores/enwiki"
    params = {"models": "articlequality", "revids": revids}

    try:
        time.sleep(0.5)  # Add delay between API calls
        response = session.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        json_response = response.json()
        if verbose:
            logger.debug(f"Response for revids {revids}: {json_response}")
        return json_response["enwiki"]["scores"][f"{revids}"]["articlequality"]["score"]
    except (requests.RequestException, KeyError) as e:
        logger.error(f"Error processing revids {revids}: {e}")
        return None


def process_topic(session, topic_url_pair):
    topic, url = topic_url_pair
    if not url:
        return None

    try:
        url_topic = url.split("/wiki/")[-1]

        # Get revision ID
        history_url = (
            f"https://en.wikipedia.org/w/index.php?title={url_topic}&action=history"
        )
        html_content = get_html_content(session, history_url)
        if html_content is None:
            return None

        match = re.search(r'"wgCurRevisionId":(\d+)', html_content)
        if not match:
            logger.error(f"Cannot find revids for {topic}.")
            return None

        revids = match.group(1)
        predicted_quality = call_ORES_api(session, revids)

        if predicted_quality is None:
            return None

        return {
            "topic": topic,
            "url": url,
            "predicted_class": predicted_quality["prediction"],
            "predicted_scores": predicted_quality["probability"],
        }
    except Exception as e:
        logger.error(f"Error processing topic {topic}: {e}")
        return None


def batch_process_topics(topic_urls_dict, max_workers=8, batch_size=20):
    session = create_session()
    results = []

    # Convert dictionary items to list for batch processing
    items = list(topic_urls_dict.items())

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for i in range(0, len(items), batch_size):
            batch = items[i : i + batch_size]

            # Add delay between batches
            if i > 0:
                time.sleep(3)  # 3 second delay between batches

            process_with_session = partial(process_topic, session)
            future_to_topic = {
                executor.submit(process_with_session, item): item for item in batch
            }

            for future in tqdm(
                as_completed(future_to_topic),
                total=len(batch),
                desc=f"Processing batch {i//batch_size + 1}/{len(items)//batch_size + 1}",
            ):
                result = future.result()
                if result:
                    results.append(result)

    return results


def main(args):
    print_warning(args.topics_urls_json)

    # Load topics
    topics_with_urls_dict = load_json(args.topics_urls_json)

    # Process topics in parallel with reduced concurrency
    results = batch_process_topics(
        topics_with_urls_dict, max_workers=args.max_workers, batch_size=args.batch_size
    )

    # Convert results to DataFrame
    df = pd.DataFrame(results)

    # Print statistics
    print_stats(df)

    # Filter high-quality articles if requested
    if args.extract_HQ_articles:
        high_quality_classes = {"B", "GA", "FA"}
        df = df[df["predicted_class"].isin(high_quality_classes)]

    # Save results
    suffix = "_all" if "all" in os.path.basename(args.topics_urls_json) else ""

    # Save CSV
    csv_path = os.path.join(args.output_dir, f"topics_ores_scores{suffix}.csv")
    df.to_csv(csv_path, index=False)

    # Save JSON
    records = df.to_dict(orient="records")
    json_path = os.path.join(args.output_dir, f"topics_ores_scores{suffix}.json")
    dump_json(records, json_path, ensure_ascii=False)

    print(f"Files saved to:\n- CSV: {csv_path}\n- JSON: {json_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Get ORES scores for Wikipedia articles."
    )
    parser.add_argument(
        "--topics_urls_json",
        type=str,
        default=TOPICS_URLS_JSON,
        help="Path to the JSON file containing topics.",
    )
    parser.add_argument(
        "--extract-HQ-articles",
        action="store_true",
        default=True,
        help="Extract high-quality Wikipedia articles based on ORES scores.",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=DATA_DIR,
        help="Directory where to save the ORES scores JSON file.",
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=32,  # Reduced from 48
        help="Maximum number of worker threads (default: 8)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=20,  # Reduced from 100
        help="Number of topics to process in each batch (default: 20)",
    )

    args = parser.parse_args()
    print(format_args(args))

    main(args)
